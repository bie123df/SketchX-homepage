---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!-- Section: About -->
<div class="section" id="about-section">
  <h2>Welcome to SketchX</h2>
  <p>
    The ultimate vision for <strong>SketchX</strong> is to understand how seeing can be
    explained by drawing. In other words, how a better understanding of human sketch data
    can be translated into insights on how human visual systems operate — and in turn, how
    such insights can benefit computer vision and cognitive science at large.
  </p>
  <p>
    SketchX has been actively investigating all aspects of sketch research since 2012. The problems we study range from conventional tasks such as sketch recognition and sketch synthesis, to those we have pioneered, such as fine-grained sketch-based image retrieval and memory-aware forensic sketch analysis.
  </p>
  
</div>

<!-- Section: Team Members -->
<div class="section" id="team-members-section" >
  <h2>👥 Team Members</h2>
  <img src="{{ '/images/team_member.png' | relative_url }}" alt="Team Member" style="max-width: 800px; display: block; margin-top: 1em;">
</div>

<!-- Section: Publications -->
<div class="section" id="publications-section" >
  <h2>📝 Team Publications</h2>
  <ol>
<li>（AAAI）<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32894">VA-AR: Learning Velocity-Aware Action Representations with Mixture of Window Attention</a> Jiangning Wei, Lixiong Qin, Bo Yu, Tianjian Zou, Chuhan Yan, Dandan Xiao, Yang Yu, Lan Yang, Ke Li, Jun Liu, 2025</li>
<li>（AAAI）<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32240">VersaGen: Unleashing Versatile Visual Control for Text-to-Image Synthesis</a> Zhipeng Chen, Lan Yang, Yonggang Qi, Honggang Zhang, Kaiyue Pang, Ke Li, Yi-Zhe Song, 2025</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.html">Demofusion: Democratising high-resolution image generation with no $$$</a> Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Its_All_About_Your_Sketch_Democratising_Sketch_Control_in_Diffusion_CVPR_2024_paper.html">It's All About Your Sketch: Democratising Sketch Control in Diffusion Models</a> Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_How_to_Handle_Sketch-Abstraction_in_Sketch-Based_Image_Retrieval_CVPR_2024_paper.html">How to handle sketch-abstraction in sketch-based image retrieval?</a> Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Youll_Never_Walk_Alone_A_Sketch_and_Text_Duet_for_CVPR_2024_paper.html">You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval</a> Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_Doodle_Your_3D_From_Abstract_Freehand_Sketches_to_Precise_3D_CVPR_2024_paper.html">Doodle your 3d: From abstract freehand sketches to precise 3d shapes</a> Hmrishav Bandyopadhyay, Subhadeep Koley, Ayan Das, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_SketchINR_A_First_Look_into_Sketches_as_Implicit_Neural_Representations_CVPR_2024_paper.html">Sketchinr: A first look into sketches as implicit neural representations</a> Hmrishav Bandyopadhyay, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Tao Xiang, Timothy Hospedales, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Text-to-Image_Diffusion_Models_are_Great_Sketch-Photo_Matchmakers_CVPR_2024_paper.html">Text-to-image diffusion models are great sketch-photo matchmakers</a> Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="https://ieeexplore.ieee.org/abstract/document/10472065/">Bi-directional ensemble feature reconstruction network for few-shot fine-grained classification</a> Jijie Wu, Dongliang Chang, Aneeshan Sain, Xiaoxu Li, Zhanyu Ma, Jie Cao, Jun Guo, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Qu_Wired_Perspectives_Multi-View_Wire_Art_Embraces_Generative_AI_CVPR_2024_paper.html">Wired perspectives: Multi-view wire art embraces generative ai</a> Zhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Chen_DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_CVPR_2024_paper.html">Democaricature: Democratising caricature generation with a rough sketch</a> Dar-Yen Chen, Ayan Kumar Bhunia, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song, 2024</li>
<li>（ECCV）<a href="https://link.springer.com/chapter/10.1007/978-3-031-72673-6_23">PartCraft: Crafting Creative Objects by Parts</a> Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang, 2024</li>
<li>（CVPR）<a href="https://ieeexplore.ieee.org/abstract/document/10471272/">Creativeseg: Semantic segmentation of creative sketches</a> Yixiao Zheng, Kaiyue Pang, Ayan Das, Dongliang Chang, Yi-Zhe Song, Zhanyu Ma, 2024</li>
<li>（arXiv）<a href="https://arxiv.org/abs/2405.18716">Sketchdeco: Decorating b&amp;w sketches with colour</a> Chaitat Utintu, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song, 2024</li>
<li>（CVPR）<a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_What_Sketch_Explainability_Really_Means_for_Downstream_Tasks_CVPR_2024_paper.html">What Sketch Explainability Really Means for Downstream Tasks?</a> Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song, 2024</li>
<li>（ICLR）<a href="https://openreview.net/forum?id=O2jyuo89CK">Modelling complex vector drawings with stroke-clouds</a> Alexander Ashcroft, Ayan Das, Yulia Gryaditskaya, Zhiyu Qu, Yi-Zhe Song, 2024</li>
<li>（CVPRW）<a href="https://openaccess.thecvf.com/content/CVPR2024W/FGVC11/html/Ng_ConceptHash_Interpretable_Fine-Grained_Hashing_via_Concept_Discovery_CVPRW_2024_paper.html">ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery</a> Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang, 2024</li>
<li>（ICLR）<a href="https://openreview.net/forum?id=nQsimt9atc">Ipr-nerf: Ownership verification meets neural radiance field</a> Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi-Zhe Song, Tao Xiang, 2024</li>
<li>（ECCV）<a href="https://link.springer.com/chapter/10.1007/978-3-031-72992-8_13">Do Generalised Classifiers Really Work on Human Drawn Sketches?</a> Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Ayan Kumar Bhunia, Yi-Zhe Song, 2024</li>

  </ol>
</div>

<!-- Section: Awards -->
<div class="section" id="awards-section">
  <h2>🎖 Honors and Awards</h2>
  <ul>
    <li>🏆 <strong>Best Science Paper Award</strong> at <strong>BMVC 2015</strong></li>
    <li>🤝 Collaborations with <strong>law enforcement agencies</strong></li>
    <li>🛍️ Partnerships with <strong>online retail platforms</strong></li>
    <li>27 x CVPR, 11 x ICCV, 11 x ECCV, 2 x SIGGRAPH Asia, 1 x ICLR, 1 x ICML, 1 x NeurIPS (as of July 2022)</li>
  </ul>
</div>

<!-- Section: Projects -->
<div class="section" id="projects-section" >
  <h2>🚀Projects</h2>
  projects
</div>

<!-- Section: News -->
<div class="section" id="news-section" >
  <h2>🔥 Team News</h2>
  <a class="twitter-timeline" data-height="600" href="https://twitter.com/SketchXlab?ref_src=twsrc%5Etfw">Tweets by SketchXlab</a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>


<!-- 页面切换脚本 -->
<script>
  function showSection(id) {
    const sections = document.querySelectorAll(".section");
    sections.forEach(section => {
      if (section.id === id) {
        // 激活新 section
        section.classList.add("active");
      } else if (section.classList.contains("active")) {
        // 淡出旧 section 后再移除 active
        section.classList.remove("active");
      }
    });

    // 更新地址栏的 hash
    if (id.endsWith("-section")) {
      const hash = id.replace("-section", "");
      history.replaceState(null, "", "#" + hash);
    }
  }

  document.addEventListener("DOMContentLoaded", function () {
    const hash = window.location.hash.replace("#", "") || "about";
    const sectionId = hash + "-section";
    showSection(sectionId);
  });
</script>



