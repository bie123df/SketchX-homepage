---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!-- Section: About -->
<div class="section" id="about-section">
  <h2>Welcome to SketchX</h2>
  <p>
    The ultimate vision for <strong>SketchX</strong> is to understand how seeing can be
    explained by drawing. In other words, how a better understanding of human sketch data
    can be translated into insights on how human visual systems operate ‚Äî and in turn, how
    such insights can benefit computer vision and cognitive science at large.
  </p>
  <p>
    SketchX has been actively investigating all aspects of sketch research since 2012. The problems we study range from conventional tasks such as sketch recognition and sketch synthesis, to those we have pioneered, such as fine-grained sketch-based image retrieval and memory-aware forensic sketch analysis.
  </p>
</div>

<!-- Section: Team Members -->
<div class="section" id="team-members-section" >
  <h2>üë• Team Members</h2>
  <img src="{{ '/images/team_member.png' | relative_url }}" alt="Team Member" style="max-width: 800px; display: block; margin-top: 1em;">
</div>

<!-- Section: Publications -->
<div class="section" id="publications-section" >
  <h2>üìù Team Publications</h2>
  <ol>
<li><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Li_Deeper_Broader_and_ICCV_2017_paper.html">Deeper, Broader and Artier Domain Generalization</a><br><em>Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M. Hospedales</em></li>

<li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/11596">Learning to generalize: Meta-learning for domain generalization</a><br><em>Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M Hospedales</em></li>

<li><a href="http://openaccess.thecvf.com/content_ICCV_2019/html/Li_Episodic_Training_for_Domain_Generalization_ICCV_2019_paper.html">Episodic training for domain generalization</a><br><em>Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe Song, Timothy M Hospedales</em></li>

<li><a href="https://link.springer.com/chapter/10.1007/978-3-030-58565-5_10">Fine-grained visual classification via progressive multi-granularity training of jigsaw patches</a><br><em>Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Yi-Zhe Song, Zhanyu Ma, Jun Guo</em></li>

<li><a href="https://ieeexplore.ieee.org/abstract/document/9005389/">The devil is in the channels: Mutual-channel loss for fine-grained image classification</a><br><em>Dongliang Chang, Yifeng Ding, Jiyang Xie, Ayan Kumar Bhunia, Xiaoxu Li, Zhanyu Ma, Ming Wu, Jun Guo, Yi-Zhe Song</em></li>

<li><a href="https://ieeexplore.ieee.org/abstract/document/9609630/">Fine-grained image analysis with deep learning: A survey</a><br><em>Xiu-Shen Wei, Yi-Zhe Song, Oisin Mac Aodha, Jianxin Wu, Yuxin Peng, Jinhui Tang, Jian Yang, Serge Belongie</em></li>

<li><a href="https://link.springer.com/article/10.1007/s11263-016-0932-3">Sketch-a-net: A deep neural network that beats humans</a><br><em>Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</em></li>

<li><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Song_Deep_Spatial-Semantic_Attention_ICCV_2017_paper.html">Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval</a><br><em>Jifei Song, Yu Qian, Yi-Zhe Song, Tao Xiang, Timothy Hospedales</em></li>

<li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Song_Generalizable_Person_Re-Identification_by_Domain-Invariant_Mapping_Network_CVPR_2019_paper.html">Generalizable person re-identification by domain-invariant mapping network</a><br><em>Jifei Song, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</em></li>

<li><a href="https://arxiv.org/abs/1501.07873">Sketch-a-net that beats humans</a><br><em>Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales</em></li>

<li><a href="https://ieeexplore.ieee.org/abstract/document/7532801/">Sketch-based image retrieval via siamese convolutional neural network</a><br><em>Yonggang Qi, Yi-Zhe Song, Honggang Zhang, Jun Liu</em></li>

<li><a href="http://openaccess.thecvf.com/content/ICCV2021/html/Lu_Simpler_Is_Better_Few-Shot_Semantic_Segmentation_With_Classifier_Weight_Transformer_ICCV_2021_paper.html">Simpler is better: Few-shot semantic segmentation with classifier weight transformer</a><br><em>Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, Tao Xiang</em></li>

<li><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Dey_Doodle_to_Search_Practical_Zero-Shot_Sketch-Based_Image_Retrieval_CVPR_2019_paper.html">Doodle to search: Practical zero-shot sketch-based image retrieval</a><br><em>Sounak Dey, Pau Riba, Anjan Dutta, Josep Llados, Yi-Zhe Song</em></li>

<li><a href="https://www.sciencedirect.com/science/article/pii/S0925231213006309">Text extraction from natural scene image: A survey</a><br><em>Honggang Zhang, Kaili Zhao, Yi-Zhe Song, Jun Guo</em></li>

<li><a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Lu_Stochastic_Classifiers_for_Unsupervised_Domain_Adaptation_CVPR_2020_paper.html">Stochastic classifiers for unsupervised domain adaptation</a><br><em>Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, Tao Xiang</em></li>

<li><a href="https://ieeexplore.ieee.org/abstract/document/9706366/">Deep learning for free-hand sketch: A survey</a><br><em>Peng Xu, Timothy M Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, Liang Wang</em></li>

<li><a href="http://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html">Your "Flamingo" is My "Bird": Fine-Grained, or Not</a><br><em>Dongliang Chang, Kaiyue Pang, Yixiao Zheng, Zhanyu Ma, Yi-Zhe Song, Jun Guo</em></li>

<li><a href="http://openaccess.thecvf.com/content/CVPR2022/html/He_Style-Based_Global_Appearance_Flow_for_Virtual_Try-On_CVPR_2022_paper.html">Style-based global appearance flow for virtual try-on</a><br><em>Sen He, Yi-Zhe Song, Tao Xiang</em></li>

<li><a href="https://proceedings.neurips.cc/paper/2021/hash/cbcb58ac2e496207586df2854b17995f-Abstract.html">One loss for all: Deep hashing with a single cosine similarity based learning objective</a><br><em>Jiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-Zhe Song, Tao Xiang</em></li>

<li><a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.html">Sketchmate: Deep hashing for million-scale human sketch retrieval</a><br><em>Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, Zhanyu Ma, Jun Guo</em></li>

  </ol>
</div>

<!-- Section: Awards -->
<div class="section" id="awards-section">
  <h2>üéñ Honors and Awards</h2>
  <ul>
    <li>üèÜ <strong>Best Science Paper Award</strong> at <strong>BMVC 2015</strong></li>
    <li>ü§ù Collaborations with <strong>law enforcement agencies</strong></li>
    <li>üõçÔ∏è Partnerships with <strong>online retail platforms</strong></li>
    <li>27 x CVPR, 11 x ICCV, 11 x ECCV, 2 x SIGGRAPH Asia, 1 x ICLR, 1 x ICML, 1 x NeurIPS (as of July 2022)</li>
  </ul>
</div>

<!-- Section: Projects -->
<div class="section" id="projects-section" >
  <h2>Projects</h2>
  projects-list
</div>

<!-- Section: News -->
<div class="section" id="news-section" >
  <h2>üî• Team News</h2>
  <a class="twitter-timeline" data-height="600" href="https://twitter.com/SketchXlab?ref_src=twsrc%5Etfw">Tweets by SketchXlab</a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>


<!-- È°µÈù¢ÂàáÊç¢ËÑöÊú¨ -->
<script>
  function showSection(id) {
    const sections = document.querySelectorAll(".section");
    sections.forEach(section => {
      if (section.id === id) {
        // ÊøÄÊ¥ªÊñ∞ section
        section.classList.add("active");
      } else if (section.classList.contains("active")) {
        // Ê∑°Âá∫Êóß section ÂêéÂÜçÁßªÈô§ active
        section.classList.remove("active");
      }
    });

    // Êõ¥Êñ∞Âú∞ÂùÄÊ†èÁöÑ hash
    if (id.endsWith("-section")) {
      const hash = id.replace("-section", "");
      history.replaceState(null, "", "#" + hash);
    }
  }

  document.addEventListener("DOMContentLoaded", function () {
    const hash = window.location.hash.replace("#", "") || "about";
    const sectionId = hash + "-section";
    showSection(sectionId);
  });
</script>



