---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!-- Section: About -->
<div class="section" id="about-section" style="font-family: sans-serif;">
  <h2 style="border-left: 6px solid #4CAF50; padding-left: 10px;">üåü Welcome to SketchX</h2>

  <p>
    The ultimate vision for <strong>SketchX</strong> is to understand how seeing can be
    explained by drawing. In other words, how a better understanding of human sketch data
    can be translated into insights on how human visual systems operate ‚Äî and in turn, how
    such insights can benefit computer vision and cognitive science at large.
  </p>
  <p>
    SketchX has been actively investigating all aspects of sketch research since 2012. The problems we study range from conventional tasks such as sketch recognition and sketch synthesis, to those we have pioneered, such as fine-grained sketch-based image retrieval and memory-aware forensic sketch analysis.
  </p>

  <!-- ‰∫∫Áâ©‰ªãÁªçÂç°Áâá -->
  <div style="display: flex; gap: 20px; margin-top: 2em; align-items: flex-start; background: #f9f9f9; padding: 20px; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.1);">
    <div style="flex-shrink: 0;">
      <img src="{{ '/images/song.jpg' | relative_url }}"
           alt="Professor Yi-Zhe Song"
           style="width: 180px; border-radius: 10px; border: 3px solid #4CAF50;">
      <p style="text-align: center; margin-top: 0.5em; font-weight: bold;">Prof. Yi-Zhe Song</p>
    </div>

    <div style="flex-grow: 1;">
      <h3 style="margin-top: 0; color: #333;">üë®‚Äçüè´ About Professor Yi-Zhe Song</h3>
      <p>
        Yi-Zhe Song is a Professor of Computer Vision and Machine Learning at the Centre for Vision Speech and Signal Processing (CVSSP), one of the UK's oldest and largest research centres on Artificial Intelligence.
      </p>
      <p>
        He leads the SketchX Lab within CVSSP ‚Äî a large research group of 3 academics, 2 postdocs, and 14 full-time PhD students. His vision for SketchX is understanding how seeing can be explained by drawing.
      </p>
      <p>
        He is an Associate Editor of the IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), and a Programme Chair for BMVC 2021. He also served as Area Chair for ECCV‚Äô22, CVPR‚Äô22, and ICCV‚Äô21.
      </p>
      <p>
        SketchX publishes in top-tier venues such as CVPR, ICCV, ECCV, SIGGRAPH Asia, and ICML, including a Best Paper Award at BMVC 2015.
      </p>
      <p>
        He founded the MSc in AI programme at Surrey, and previously did the same at Queen Mary University of London.
      </p>
      <p>
        He received his PhD from the University of Bath (2008), MSc from Cambridge (2004, Best Dissertation Award), and BSc from Bath (First Class Honours).
      </p>
      <p>
        He is a Senior Member of IEEE, a Fellow of the HEA, and a full EPSRC review college member. He also reviews for international bodies like the Czech Science Foundation and S√£o Paulo Research Foundation.
      </p>
    </div>
  </div>
</div>



<!-- Section: Team Members -->
<div class="section" id="team-members-section" style="margin-top: 3em;">
  <h2 style="border-left: 6px solid #4CAF50; padding-left: 10px;">üë• Team Members</h2>

  <div style="background: #f9f9f9; padding: 20px; border-radius: 12px; box-shadow: 0 4px 12px rgba(0,0,0,0.05); text-align: center;">
    <img src="{{ '/images/team_member.png' | relative_url }}"
         alt="Team Member"
         style="max-width: 90%; border-radius: 10px; border: 2px solid #ddd; margin-top: 1em;">
    <p style="margin-top: 1em; font-style: italic; color: #555;">Team structure overview</p>
  </div>
</div>

<!-- Section: Publications -->
<div class="section" id="publications-section" >
  <h2>üìù Team Publications</h2>
  <ol>

<ol>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #f44336; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">AAAI 2025</span><br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32894" target="_blank">VA-AR: Learning Velocity-Aware Action Representations with Mixture of Window Attention</a><br>
      <small>Jiangning Wei, Lixiong Qin, Bo Yu, Tianjian Zou, Chuhan Yan, Dandan Xiao, Yang Yu, Lan Yang, Ke Li, Jun Liu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #f44336; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">AAAI 2025</span><br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32240" target="_blank">VersaGen: Unleashing Versatile Visual Control for Text-to-Image Synthesis</a><br>
      <small>Zhipeng Chen, Lan Yang, Yonggang Qi, Honggang Zhang, Kaiyue Pang, Ke Li, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Du_DemoFusion_Democratising_High-Resolution_Image_Generation_With_No__CVPR_2024_paper.html" target="_blank">Demofusion: Democratising high-resolution image generation with no $$$</a><br>
      <small>Ruoyi Du, Dongliang Chang, Timothy Hospedales, Yi-Zhe Song, Zhanyu Ma</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Its_All_About_Your_Sketch_Democratising_Sketch_Control_in_Diffusion_CVPR_2024_paper.html" target="_blank">It's All About Your Sketch: Democratising Sketch Control in Diffusion Models</a><br>
      <small>Subhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_How_to_Handle_Sketch-Abstraction_in_Sketch-Based_Image_Retrieval_CVPR_2024_paper.html" target="_blank">How to handle sketch-abstraction in sketch-based image retrieval?</a><br>
      <small>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Youll_Never_Walk_Alone_A_Sketch_and_Text_Duet_for_CVPR_2024_paper.html" target="_blank">You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval</a><br>
      <small>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_Doodle_Your_3D_From_Abstract_Freehand_Sketches_to_Precise_3D_CVPR_2024_paper.html" target="_blank">Doodle your 3d: From abstract freehand sketches to precise 3d shapes</a><br>
      <small>Hmrishav Bandyopadhyay, Subhadeep Koley, Ayan Das, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_SketchINR_A_First_Look_into_Sketches_as_Implicit_Neural_Representations_CVPR_2024_paper.html" target="_blank">Sketchinr: A first look into sketches as implicit neural representations</a><br>
      <small>Hmrishav Bandyopadhyay, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Tao Xiang, Timothy Hospedales, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Koley_Text-to-Image_Diffusion_Models_are_Great_Sketch-Photo_Matchmakers_CVPR_2024_paper.html" target="_blank">Text-to-image diffusion models are great sketch-photo matchmakers</a><br>
      <small>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/10472065/" target="_blank">Bi-directional ensemble feature reconstruction network for few-shot fine-grained classification</a><br>
      <small>Jijie Wu, Dongliang Chang, Aneeshan Sain, Xiaoxu Li, Zhanyu Ma, Jie Cao, Jun Guo, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Qu_Wired_Perspectives_Multi-View_Wire_Art_Embraces_Generative_AI_CVPR_2024_paper.html" target="_blank">Wired perspectives: Multi-view wire art embraces generative ai</a><br>
      <small>Zhiyu Qu, Lan Yang, Honggang Zhang, Tao Xiang, Kaiyue Pang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Chen_DemoCaricature_Democratising_Caricature_Generation_with_a_Rough_Sketch_CVPR_2024_paper.html" target="_blank">Democaricature: Democratising caricature generation with a rough sketch</a><br>
      <small>Dar-Yen Chen, Ayan Kumar Bhunia, Subhadeep Koley, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2024</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-72673-6_23" target="_blank">PartCraft: Crafting Creative Objects by Parts</a><br>
      <small>Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/10471272/" target="_blank">Creativeseg: Semantic segmentation of creative sketches</a><br>
      <small>Yixiao Zheng, Kaiyue Pang, Ayan Das, Dongliang Chang, Yi-Zhe Song, Zhanyu Ma</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2024</span><br>
      <a href="https://arxiv.org/abs/2405.18716" target="_blank">Sketchdeco: Decorating b&amp;w sketches with colour</a><br>
      <small>Chaitat Utintu, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2024/html/Bandyopadhyay_What_Sketch_Explainability_Really_Means_for_Downstream_Tasks_CVPR_2024_paper.html" target="_blank">What Sketch Explainability Really Means for Downstream Tasks?</a><br>
      <small>Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICLR 2024</span><br>
      <a href="https://openreview.net/forum?id=O2jyuo89CK" target="_blank">Modelling complex vector drawings with stroke-clouds</a><br>
      <small>Alexander Ashcroft, Ayan Das, Yulia Gryaditskaya, Zhiyu Qu, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPRW 2024</span><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2024W/FGVC11/html/Ng_ConceptHash_Interpretable_Fine-Grained_Hashing_via_Concept_Discovery_CVPRW_2024_paper.html" target="_blank">ConceptHash: Interpretable Fine-Grained Hashing via Concept Discovery</a><br>
      <small>Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICLR 2024</span><br>
      <a href="https://openreview.net/forum?id=nQsimt9atc" target="_blank">Ipr-nerf: Ownership verification meets neural radiance field</a><br>
      <small>Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2024</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-72992-8_13" target="_blank">Do Generalised Classifiers Really Work on Human Drawn Sketches?</a><br>
      <small>Hmrishav Bandyopadhyay, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Ayan Kumar Bhunia, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICLR 2024</span><br>
      <a href="https://openreview.net/forum?id=5xadJmgwix" target="_blank">Scale-Adaptive Diffusion Model for Complex Sketch Synthesis</a><br>
      <small>Jijin Hu, Ke Li, Yonggang Qi, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICLR 2024</span><br>
      <a href="https://openreview.net/forum?id=5xadJmgwix" target="_blank">Scale-Adaptive Diffusion Model for Complex Sketch Synthesis</a><br>
      <small>Jijin Hu, Ke Li, Yonggang Qi, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2024</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-72998-0_9" target="_blank">Freeview Sketching: View-Aware Fine-Grained Sketch-Based Image Retrieval</a><br>
      <small>Aneeshan Sain, Pinaki Nath Chowdhury, Subhadeep Koley, Ayan Kumar Bhunia, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/10707331/" target="_blank">Understanding Episode Hardness in Few-Shot Learning</a><br>
      <small>Yurong Guo, Ruoyi Du, Aneeshan Sain, Kongming Liang, Yuan Dong, Yi-Zhe Song, Zhanyu Ma</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #795548; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IJCV 2024</span><br>
      <a href="https://link.springer.com/article/10.1007/s11263-024-02001-1" target="_blank">Annotation-Free Human Sketch Quality Assessment</a><br>
      <small>Lan Yang, Kaiyue Pang, Honggang Zhang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2024</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/10589301/" target="_blank">3D Reconstruction From a Single Sketch via View-Dependent Depth Sampling</a><br>
      <small>Chenjian Gao, Xilin Wang, Qian Yu, Lu Sheng, Jing Zhang, Xiaoguang Han, Yi-Zhe Song, Dong Xu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICANN 2024</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-72353-7_14" target="_blank">Advancing Free-Breathing Cardiac Cine MRI: Retrospective Respiratory Motion Correction Via Kspace-and-Image Guided Diffusion Model</a><br>
      <small>Hongming Guo, Ziqing Huang, Qian Yuan, Hanbo Song, Zhiyan Liu, Xianzhao Feng, Anqi Liu, Min Liu, Ke Li, Ruixi Zhou</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICANN 2024</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-78347-0_3" target="_blank">Sketch2Seg: Sketch-Based Image Segmentation with Pre-trained Diffusion Model</a><br>
      <small>Xin Dai, Haoge Deng, Ke Li, Yonggang Qi</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Sain_CLIP_for_All_Things_Zero-Shot_Sketch-Based_Image_Retrieval_Fine-Grained_or_CVPR_2023_paper.html" target="_blank">Clip for all things zero-shot sketch-based image retrieval, fine-grained or not</a><br>
      <small>Aneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Subhadeep Koley, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #f44336; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">AAAI 2023</span><br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/25383" target="_blank">Bi-directional feature reconstruction network for fine-grained few-shot image classification</a><br>
      <small>Jijie Wu, Dongliang Chang, Aneeshan Sain, Xiaoxu Li, Zhanyu Ma, Jie Cao, Jun Guo, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #ff9800; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">NeurIPS 2023</span><br>
      <a href="https://proceedings.neurips.cc/paper_files/paper/2023/hash/0fb98d483fa580e0354bcdd3a003a3f3-Abstract-Conference.html" target="_blank">Headsculpt: Crafting 3d head avatars with text</a><br>
      <small>Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, Kwan-Yee K Wong</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Koley_Picture_That_Sketch_Photorealistic_Image_Generation_From_Abstract_Sketches_CVPR_2023_paper.html" target="_blank">Picture that sketch: Photorealistic image generation from abstract sketches</a><br>
      <small>Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Zero-Shot_Everything_Sketch-Based_Image_Retrieval_and_in_Explainable_Style_CVPR_2023_paper.html" target="_blank">Zero-shot everything sketch-based image retrieval, and in explainable style</a><br>
      <small>Fengyin Lin, Mingkang Li, Da Li, Timothy Hospedales, Yi-Zhe Song, Yonggang Qi</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2023/html/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.html" target="_blank">Difftad: Temporal action detection with proposal denoising diffusion</a><br>
      <small>Sauradip Nag, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Han_FAME-ViL_Multi-Tasking_Vision-Language_Model_for_Heterogeneous_Fashion_Tasks_CVPR_2023_paper.html" target="_blank">Fame-vil: Multi-tasking vision-language model for heterogeneous fashion tasks</a><br>
      <small>Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Chowdhury_SceneTrilogy_On_Human_Scene-Sketch_and_Its_Complementarity_With_Photo_and_CVPR_2023_paper.html" target="_blank">Scenetrilogy: On human scene-sketch and its complementarity with photo and text</a><br>
      <small>Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICLR 2023</span><br>
      <a href="https://openreview.net/forum?id=4eJ43EN2g6l" target="_blank">SketchKnitter: Vectorized Sketch Generation with Diffusion Models</a><br>
      <small>Qiang Wang, Haoge Deng, Yonggang Qi, Da Li, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2023/html/Han_Controllable_Person_Image_Synthesis_with_Pose-Constrained_Latent_Diffusion_ICCV_2023_paper.html" target="_blank">Controllable person image synthesis with pose-constrained latent diffusion</a><br>
      <small>Xiao Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">TMM 2023</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/10145054/" target="_blank">Prediction calibration for generalized few-shot semantic segmentation</a><br>
      <small>Zhihe Lu, Sen He, Da Li, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Sain_Exploiting_Unlabelled_Photos_for_Stronger_Fine-Grained_SBIR_CVPR_2023_paper.html" target="_blank">Exploiting unlabelled photos for stronger fine-grained SBIR</a><br>
      <small>Aneeshan Sain, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Soumitri Chattopadhyay, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.html" target="_blank">Photo Pre-Training, But for Sketch</a><br>
      <small>, Ke Li, Kaiyue Pang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2023/html/Li_Photo_Pre-Training_but_for_Sketch_CVPR_2023_paper.html" target="_blank">Photo Pre-Training, But for Sketch</a><br>
      <small>, Ke Li, Kaiyue Pang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2023</span><br>
      <a href="https://arxiv.org/abs/2303.05734" target="_blank">Generative Model Based Noise Robust Training for Unsupervised Domain Adaptation</a><br>
      <small>, Zhongying Deng, Da Li, Junjun He, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2023</span><br>
      <a href="https://ui.adsabs.harvard.edu/abs/2023arXiv230315149N/abstract" target="_blank">What Can Human Sketches Do for Object Detection?</a><br>
      <small>, Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2023</span><br>
      <a href="https://ui.adsabs.harvard.edu/abs/2023arXiv231115477W/abstract" target="_blank">DreamCreature: Crafting Photorealistic Virtual Creatures from Imagination</a><br>
      <small>, Kam Woh Ng, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2023</span><br>
      <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Li_Photo_Pre-Training_but_CVPR_2023_supplemental.pdf" target="_blank">Supplementary material: Photo Pre-Training, But for Sketch</a><br>
      <small>, Ke Li, Kaiyue Pang, Yi-Zhe Song, CVSSP SketchX</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">TNNLS 2022</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9706366/" target="_blank">Deep learning for free-hand sketch: A survey</a><br>
      <small>, Peng Xu, Timothy M Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, Liang Wang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2022</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2022/html/He_Style-Based_Global_Appearance_Flow_for_Virtual_Try-On_CVPR_2022_paper.html" target="_blank">Style-based global appearance flow for virtual try-on</a><br>
      <small>, Sen He, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-20062-5_39" target="_blank">Zero-shot temporal action detection via vision-language prompting</a><br>
      <small>, Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #f44336; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">AAAI 2022</span><br>
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20226" target="_blank">Hybrid graph neural networks for few-shot learning</a><br>
      <small>, Tianyuan Yu, Sen He, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2022</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2022/html/Bhunia_Sketching_Without_Worrying_Noise-Tolerant_Sketch-Based_Image_Retrieval_CVPR_2022_paper.html" target="_blank">Sketching without worrying: Noise-tolerant sketch-based image retrieval</a><br>
      <small>, Ayan Kumar Bhunia, Subhadeep Koley, Abdullah Faiz Ur Rahman Khilji, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-20062-5_37" target="_blank">Proposal-free temporal action detection via global segmentation mask learning</a><br>
      <small>, Sauradip Nag, Xiatian Zhu, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2022</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2022/html/Sain_Sketch3T_Test-Time_Training_for_Zero-Shot_SBIR_CVPR_2022_paper.html" target="_blank">Sketch3t: Test-time training for zero-shot sbir</a><br>
      <small>, Aneeshan Sain, Ayan Kumar Bhunia, Vaishnav Potlapalli, Pinaki Nath Chowdhury, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-19833-5_37" target="_blank">Fashionvil: Fashion-focused vision-and-language representation learning</a><br>
      <small>, Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #607d8b; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">TIP 2022</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9813442/" target="_blank">Dynamic instance domain adaptation</a><br>
      <small>, Zhongying Deng, Kaiyang Zhou, Da Li, Junjun He, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2022</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2022/html/Bhunia_Doodle_It_Yourself_Class_Incremental_Learning_by_Drawing_a_Few_CVPR_2022_paper.html" target="_blank">Doodle it yourself: Class incremental learning by drawing a few sketches</a><br>
      <small>, Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Subhadeep Koley, Rohit Kundu, Aneeshan Sain, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-19836-6_10" target="_blank">Adaptive fine‚Äëgrained sketch‚Äëbased image retrieval</a><br>
      <small>, Ayan Kumar Bhunia, Aneeshan Sain, Parth Hiren Shah, Animesh Gupta, Pinaki Nath Chowdhury, Tao Xiang, Yi‚ÄëZhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-20062-5_38" target="_blank">Semi‚Äësupervised temporal action detection with proposal‚Äëfree masking</a><br>
      <small>, Sauradip Nag, Xiatian Zhu, Yi‚ÄëZhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2022</span><br>
      <a href="https://link.springer.com/chapter/10.1007/978-3-031-19769-7_27" target="_blank">Sketchsampler: Sketch‚Äëbased 3d reconstruction via view‚Äëdependent depth sampling</a><br>
      <small>, Chenjian Gao, Qian Yu, Lu Sheng, Yi‚ÄëZhe Song, Dong Xu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ArXiv 2022</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9739995/" target="_blank">One sketch for all: One‚Äëshot personalized sketch segmentation</a><br>
      <small>, Anran Qi, Yulia Gryaditskaya, Tao Xiang, Yi‚ÄëZhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Chang_Your_Flamingo_is_My_Bird_Fine-Grained_or_Not_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Your "Flamingo" is My "Bird": Fine-Grained, or Not</a><br>
      <small>Dongliang Chang, Kaiyue Pang, Yixiao Zheng, Zhanyu Ma, Yi-Zhe Song, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Sain_StyleMeUp_Towards_Style-Agnostic_Sketch-Based_Image_Retrieval_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Stylemeup: Towards style-agnostic sketch-based image retrieval</a><br>
      <small>Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9609669/" target="_blank" rel="noopener noreferrer" target="_blank">Progressive learning of category-consistent multi-granularity features for fine-grained visual classification</a><br>
      <small>Ruoyi Du, Jiyang Xie, Zhanyu Ma, Dongliang Chang, Yi-Zhe Song, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_More_Photos_Are_All_You_Need_Semi-Supervised_Learning_for_Fine-Grained_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">More photos are all you need: Semi-supervised learning for fine-grained sketch based image retrieval</a><br>
      <small>Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Yongxin Yang, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Bhunia_Joint_Visual_Semantic_Reasoning_Multi-Stage_Decoder_for_Text_Recognition_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Joint visual semantic reasoning: Multi-stage decoder for text recognition</a><br>
      <small>Ayan Kumar Bhunia, Aneeshan Sain, Amandeep Kumar, Shuvozit Ghose, Pinaki Nath Chowdhury, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_Vectorization_and_Rasterization_Self-Supervised_Learning_for_Sketch_and_Handwriting_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Vectorization and rasterization: Self-supervised learning for sketch and handwriting</a><br>
      <small>Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Yongxin Yang, Timothy M Hospedales, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/He_Context-Aware_Layout_to_Image_Generation_With_Enhanced_Object_Appearance_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Context-aware layout to image generation with enhanced object appearance</a><br>
      <small>Sen He, Wentong Liao, Michael Ying Yang, Yongxin Yang, Yi-Zhe Song, Bodo Rosenhahn, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Chen_Variational_Attention_Propagating_Domain-Specific_Knowledge_for_Multi-Domain_Learning_in_Crowd_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Variational attention: Propagating domain-specific knowledge for multi-domain learning in crowd counting</a><br>
      <small>Binghui Chen, Zhaoyi Yan, Ke Li, Pengyu Li, Biao Wang, Wangmeng Zuo, Lei Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #795548; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IJCV 2021</span><br>
      <a href="https://link.springer.com/article/10.1007/s11263-020-01382-3" target="_blank" rel="noopener noreferrer" target="_blank">Fine-grained instance-level sketch-based image retrieval</a><br>
      <small>Qian Yu, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9573376/" target="_blank" rel="noopener noreferrer" target="_blank">Toward fine-grained sketch-based 3D shape retrieval</a><br>
      <small>Anran Qi, Yulia Gryaditskaya, Jifei Song, Yongxin Yang, Yonggang Qi, Timothy M Hospedales, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Bhunia_MetaHTR_Towards_Writer-Adaptive_Handwritten_Text_Recognition_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Metahtr: Towards writer-adaptive handwritten text recognition</a><br>
      <small>Ayan Kumar Bhunia, Shuvozit Ghose, Amandeep Kumar, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Das_Cloud2Curve_Generation_and_Vectorization_of_Parametric_Sketches_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Cloud2curve: Generation and vectorization of parametric sketches</a><br>
      <small>Ayan Das, Yongxin Yang, Timothy M Hospedales, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/He_Disentangled_Lifespan_Face_Synthesis_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Disentangled lifespan face synthesis</a><br>
      <small>Sen He, Wentong Liao, Michael Ying Yang, Yi-Zhe Song, Bodo Rosenhahn, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Bhunia_Text_Is_Text_No_Matter_What_Unifying_Text_Recognition_Using_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Text is text, no matter what: Unifying text recognition using knowledge distillation</a><br>
      <small>Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Yang_SketchAA_Abstract_Representation_for_Abstract_Sketches_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Sketchaa: Abstract representation for abstract sketches</a><br>
      <small>Lan Yang, Kaiyue Pang, Honggang Zhang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Qi_SketchLattice_Latticed_Representation_for_Sketch_Manipulation_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Sketchlattice: Latticed representation for sketch manipulation</a><br>
      <small>Yonggang Qi, Guoyao Su, Pinaki Nath Chowdhury, Mingkang Li, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/ICCV2021/html/Bhunia_Towards_the_Unseen_Iterative_Text_Recognition_by_Distilling_From_Errors_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Towards the unseen: Iterative text recognition by distilling from errors</a><br>
      <small>Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">3DV 2021</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/9665875/" target="_blank" rel="noopener noreferrer" target="_blank">Fine-grained vr sketching: Dataset and insights</a><br>
      <small>Ling Luo, Yulia Gryaditskaya, Yongxin Yang, Tao Xiang, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2021</span><br>
      <a href="http://openaccess.thecvf.com/content/CVPR2021/html/Qi_PQA_Perceptual_Question_Answering_CVPR_2021_paper.html" target="_blank" rel="noopener noreferrer" target="_blank">Pqa: Perceptual question answering</a><br>
      <small>Yonggang Qi, Kai Zhang, Aneeshan Sain, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2021</span><br>
      <a href="https://arxiv.org/abs/2105.08237" target="_blank" rel="noopener noreferrer" target="_blank">Towards unsupervised sketch-based image retrieval</a><br>
      <small>Conghui Hu, Yongxin Yang, Yunpeng Li, Timothy M Hospedales, Yi-Zhe Song</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Xu_SketchMate_Deep_Hashing_CVPR_2018_paper.html" target="_blank">Sketchmate: Deep hashing for million-scale human sketch retrieval</a><br>
      <small>, Peng Xu, Yongye Huang, Tongtong Yuan, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, Zhanyu Ma, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Song_Learning_to_Sketch_CVPR_2018_paper.html" target="_blank">Learning to sketch with shortcut cycle consistency</a><br>
      <small>, Jifei Song, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">TNNLS 2018</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/8401709/" target="_blank">Variational Bayesian learning for Dirichlet process mixture of inverted Dirichlet distributions in non-Gaussian image feature modeling</a><br>
      <small>, Zhanyu Ma, Yuping Lai, W Bastiaan Kleijn, Yi-Zhe Song, Liang Wang, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Muhammad_Learning_Deep_Sketch_CVPR_2018_paper.html" target="_blank">Learning deep sketch abstraction</a><br>
      <small>, Umar Riaz Muhammad, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Changqing_Zou_SketchyScene_Richly-Annotated_Scene_ECCV_2018_paper.html" target="_blank">Sketchyscene: Richly-annotated scene sketches</a><br>
      <small>, Changqing Zou, Qian Yu, Ruofei Du, Haoran Mo, Yi-Zhe Song, Tao Xiang, Chengying Gao, Baoquan Chen, Hao Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ACM 2018</span><br>
      <a href="https://dl.acm.org/doi/abs/10.1145/3240508.3240606" target="_blank">Cross-domain adversarial feature learning for sketch re-identification</a><br>
      <small>, Lu Pang, Yaowei Wang, Yi-Zhe Song, Tiejun Huang, Yonghong Tian</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Ke_LI_Universal_Sketch_Perceptual_ECCV_2018_paper.html" target="_blank">Universal sketch perceptual grouping</a><br>
      <small>, Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, Honggang Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Ke_LI_Universal_Sketch_Perceptual_ECCV_2018_paper.html" target="_blank">Universal sketch perceptual grouping</a><br>
      <small>, Ke Li, Kaiyue Pang, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, Honggang Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2018</span><br>
      <a href="https://scholar.archive.org/work/i7rkm2e5abexnn7ekmbthedx2e/access/wayback/http://bmvc2018.org/contents/papers/0040.pdf" target="_blank">Semantic Embedding for Sketch-Based 3D Shape Retrieval</a><br>
      <small>, Anran Qi, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Sketch-a-Classifier_Sketch-Based_Photo_CVPR_2018_paper.html" target="_blank">Sketch-a-classifier: Sketch-based photo classifier generation</a><br>
      <small>, Conghui Hu, Da Li, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ECCV 2018</span><br>
      <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Kaiyue_Pang_Deep_Factorised_Inverse-Sketching_ECCV_2018_paper.html" target="_blank">Deep factorised inverse-sketching</a><br>
      <small>, Kaiyue Pang, Da Li, Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">NIPS 2018</span><br>
      <a href="https://scholar.google.com/scholar?cluster=11748417708108091907&hl=en&oi=scholarr" target="_blank">Greedy hash: Towards fast optimization for accurate hash coding in CNN</a><br>
      <small>, Jiun Tian Hoe, Kam Woh Ng, Tianyu Zhang, Chee Seng Chan, Yi-Zhe Song, Tao Xiang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IEEE Access 2018</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/8395088/" target="_blank">IEEE Access special section editorial: Recent advantages of computer vision</a><br>
      <small>, Zhanyu Ma, Haibin Ling, Yi-Zhe Song, Timothy Hospedales, Wei Jia, Yuxin Peng, Aili Han</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">Neurocomputing 2018</span><br>
      <a href="https://openresearch.surrey.ac.uk/esploro/outputs/journalArticle/Recent-advances-in-machine-learning-for/99511114602346" target="_blank">Recent advances in machine learning for non-Gaussian data processing</a><br>
      <small>, Zhanyu Ma, J-T Chien, Z-H Tan, Yi-Zhe Song, Jalil Taghia, Ming Xiao</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2017</span><br>
      <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Li_Deeper_Broader_and_ICCV_2017_paper.html" target="_blank">Deeper, Broader and Artier Domain Generalization</a><br>
      <small>, Da Li, Yongxin Yang, Yi-Zhe Song, Timothy M. Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #795548; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IJCV 2017</span><br>
      <a href="https://link.springer.com/article/10.1007/s11263-016-0932-3" target="_blank">Sketch-a-net: A deep neural network that beats humans</a><br>
      <small>, Qian Yu, Yongxin Yang, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2017</span><br>
      <a href="http://openaccess.thecvf.com/content_iccv_2017/html/Song_Deep_Spatial-Semantic_Attention_ICCV_2017_paper.html" target="_blank">Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval</a><br>
      <small>, Jifei Song, Yu Qian, Yi-Zhe Song, Tao Xiang, Timothy Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">arXiv 2017</span><br>
      <a href="https://arxiv.org/abs/1711.08106" target="_blank">The devil is in the middle: Exploiting mid-level representations for cross-domain instance matching</a><br>
      <small>, Qian Yu, Xiaobin Chang, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2017</span><br>
      <a href="https://www.research.ed.ac.uk/en/publications/cross-domain-generative-learning-for-fine-grained-sketch-based-im" target="_blank">Cross-domain Generative Learning for Fine-Grained Sketch-Based Image Retrieval</a><br>
      <small>, Kaiyue Pang, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">Neurocomputing 2017</span><br>
      <a href="https://www.sciencedirect.com/science/article/pii/S0925231217314364" target="_blank">Cross-modal Subspace Learning for Fine-grained Sketch-based Image Retrieval</a><br>
      <small>, Peng Xu, Qiyue Yin, Yongye Huang, Yi-Zhe Song, Zhanyu Ma, Liang Wang, Tao Xiang, W Bastiaan Kleijn, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2017</span><br>
      <a href="https://www.research.ed.ac.uk/en/publications/fine-grained-image-retrieval-the-textsketch-input-dilemma" target="_blank" target="_blank">Fine-Grained Image Retrieval: the Text/Sketch Input Dilemma</a><br>
      <small>Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICIP 2017</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/8016664/" target="_blank" target="_blank">Synergistic instance-level subspace alignment for fine-grained sketch-based image retrieval</a><br>
      <small>Ke Li, Kaiyue Pang, Yi-Zhe Song, Timothy M Hospedales, Tao Xiang, Honggang Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICIP 2017</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/8016664/" target="_blank" target="_blank">Synergistic instance-level subspace alignment for fine-grained sketch-based image retrieval</a><br>
      <small>Ke Li, Kaiyue Pang, Yi-Zhe Song, Timothy M Hospedales, Tao Xiang, Honggang Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2017</span><br>
      <a href="https://scholar.google.com/scholar?cluster=6337503410293765834&hl=en&oi=scholarr" target="_blank" target="_blank">Sketch-a-net that beats humans [OL]</a><br>
      <small>Q Yu, Y Yang, YZ Song, T Xiang, TM Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2017</span><br>
      <a href="https://www.research.ed.ac.uk/en/publications/now-you-see-me-deep-face-hallucination-for-unviewed-sketches" target="_blank" target="_blank">Now You See Me: Deep Face Hallucination for Unviewed Sketches</a><br>
      <small>Conghui Hu, Da Li, Yi-Zhe Song, Timothy M. Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2017</span><br>
      <a href="https://scholar.google.com/scholar?cluster=197706842303472179&hl=en&oi=scholarr" target="_blank" target="_blank">‚ÄûDeeper</a><br>
      <small>D Li, Y Yang, YZ Song, T Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICIP 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7532801/" target="_blank" target="_blank">Sketch-based image retrieval via siamese convolutional neural network</a><br>
      <small>Yonggang Qi, Yi-Zhe Song, Honggang Zhang, Jun Liu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IVC 2016</span><br>
      <a href="https://www.sciencedirect.com/science/article/pii/S0262885616301524" target="_blank" target="_blank">A survey on heterogeneous face recognition: Sketch, infra-red, 3D and low-resolution</a><br>
      <small>Shuxin Ouyang, Timothy Hospedales, Yi-Zhe Song, Xueming Li, Chen Change Loy, Xiaogang Wang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2016</span><br>
      <a href="http://openaccess.thecvf.com/content_cvpr_2016/html/Ouyang_ForgetMeNot_Memory-Aware_Forensic_CVPR_2016_paper.html" target="_blank" target="_blank">Forgetmenot: Memory-aware forensic facial sketch matching</a><br>
      <small>Shuxin Ouyang, Timothy M Hospedales, Yi-Zhe Song, Xueming Li</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #795548; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IJCV 2016</span><br>
      <a href="https://link.springer.com/article/10.1007/s11263-016-0963-9" target="_blank" target="_blank">Free-hand sketch synthesis with deformable stroke models</a><br>
      <small>Y. Li, Y-Z. Song, T. Hospedales, S. Gong</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #f44336; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">AAAI 2016</span><br>
      <a href="https://www.research.ed.ac.uk/en/publications/deep-multi-task-attribute-driven-ranking-for-fine-grained-sketch-" target="_blank" target="_blank">Deep multi-task attribute-driven ranking for fine-grained sketch-based image retrieval</a><br>
      <small>Jifei Song, Yi-Zhe Song, Tao Xiang, Timothy Hospedales, Xiang Ruan</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">WACV 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7477615/" target="_blank" target="_blank">Fine-grained sketch-based image retrieval: The role of part-aware attributes</a><br>
      <small>Ke Li, Kaiyue Pang, Yi-Zhe Song, Timothy Hospedales, Honggang Zhang, Yichuan Hu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">WACV 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7477615/" target="_blank" target="_blank">Fine-grained sketch-based image retrieval: The role of part-aware attributes</a><br>
      <small>Ke Li, Kaiyue Pang, Yi-Zhe Song, Timothy Hospedales, Honggang Zhang, Yichuan Hu</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">ICCV 2016</span><br>
      <a href="https://scholar.google.com/scholar?cluster=13550880441023903228&hl=en&oi=scholarr" target="_blank" target="_blank">Sketch me that shoe</a><br>
      <small>Yyu Qian, Liu Feng, Yi-Zhe Song, Xiang Tao, Change Loy Chen</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IC-NIDC 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7974625/" target="_blank" target="_blank">Cross-modal subspace learning for sketch-based image retrieval: A comparative study</a><br>
      <small>Peng Xu, Ke Li, Zhanyu Ma, Yi-Zhe Song, Liang Wang, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">IC-NIDC 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7974625/" target="_blank" target="_blank">Cross-modal subspace learning for sketch-based image retrieval: A comparative study</a><br>
      <small>Peng Xu, Ke Li, Zhanyu Ma, Yi-Zhe Song, Liang Wang, Jun Guo</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">VCIP 2016</span><br>
      <a href="https://ieeexplore.ieee.org/abstract/document/7805451/" target="_blank" target="_blank">Cross-modal face matching: Tackling visual abstraction using fine-grained attributes</a><br>
      <small>Yichuan Hu, Ke Li, Honggang Zhang</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #009688; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">BMVC 2015</span><br>
      <a href="https://arxiv.org/abs/1501.07873" target="_blank" target="_blank">Sketch-a-net that beats humans</a><br>
      <small>Qian Yu, Yongxin Yang, Yi-Zhe Song, Tao Xiang, Timothy Hospedales</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #555; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVIU 2015</span><br>
      <a href="https://www.sciencedirect.com/science/article/pii/S1077314215000375" target="_blank" target="_blank">Free-hand sketch recognition by multi-kernel feature learning</a><br>
      <small>Yi Li, Timothy M Hospedales, Yi-Zhe Song, Shaogang Gong</small>
    </li>
<li style="margin-bottom: 1em;">
      <span style="display: inline-block; background: #3f51b5; color: white; padding: 2px 6px; border-radius: 4px; font-size: 0.9em;">CVPR 2015</span><br>
      <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Qi_Making_Better_Use_2015_CVPR_paper.html" target="_blank" target="_blank">Making better use of edges via perceptual grouping</a><br>
      <small>Yonggang Qi, Yi-Zhe Song, Tao Xiang, Honggang Zhang, Timothy Hospedales, Yi Li, Jun Guo</small>
    </li>
</ol>


  </ol>
</div>

<!-- Section: Awards -->
<div class="section" id="awards-section">
  <h2>üéñ Honors and Awards</h2>
  <ul>
    <li>üèÜ <strong>Best Science Paper Award</strong> at <strong>BMVC 2015</strong></li>
    <li>ü§ù Collaborations with <strong>law enforcement agencies</strong></li>
    <li>üõçÔ∏è Partnerships with <strong>online retail platforms</strong></li>
    <li>27 x CVPR, 11 x ICCV, 11 x ECCV, 2 x SIGGRAPH Asia, 1 x ICLR, 1 x ICML, 1 x NeurIPS (as of July 2022)</li>
  </ul>
</div>

<!-- Section: Projects -->
<div class="section" id="projects-section" >
  <h2>üöÄProjects</h2>
  projects
</div>

<!-- Section: News -->
<div class="section" id="news-section" >
  <h2>üî• Team News</h2>
  <a class="twitter-timeline" data-height="600" href="https://twitter.com/SketchXlab?ref_src=twsrc%5Etfw">Tweets by SketchXlab</a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>


<!-- È°µÈù¢ÂàáÊç¢ËÑöÊú¨ -->
<script>
  function showSection(id) {
    const sections = document.querySelectorAll(".section");
    sections.forEach(section => {
      if (section.id === id) {
        // ÊøÄÊ¥ªÊñ∞ section
        section.classList.add("active");
      } else if (section.classList.contains("active")) {
        // Ê∑°Âá∫Êóß section ÂêéÂÜçÁßªÈô§ active
        section.classList.remove("active");
      }
    });

    // Êõ¥Êñ∞Âú∞ÂùÄÊ†èÁöÑ hash
    if (id.endsWith("-section")) {
      const hash = id.replace("-section", "");
      history.replaceState(null, "", "#" + hash);
    }
  }

  document.addEventListener("DOMContentLoaded", function () {
    const hash = window.location.hash.replace("#", "") || "about";
    const sectionId = hash + "-section";
    showSection(sectionId);
  });
</script>



